{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anmol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Anmol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Anmol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\Anmol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\treebank.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('treebank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Anmol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Anmol\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', 'sample', 'sentence', 'to', 'test', 'the', 'stopwords', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "sent = \"This is the sample sentence to test the stopwords.\"\n",
    "print(word_tokenize(sent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'the', 'sample', 'sentence', 'to', 'test', 'the', 'stopwords.']\n"
     ]
    }
   ],
   "source": [
    "print(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['God is Great!', 'I won a lottery.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"God is Great! I won a lottery.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                1    2    3    4\n",
      "'             NaN  1.0  1.0  NaN\n",
      "'Content      NaN  1.0  NaN  NaN\n",
      "'lorem        NaN  NaN  1.0  NaN\n",
      "(             NaN  NaN  NaN  1.0\n",
      ")             NaN  NaN  NaN  1.0\n",
      ",             NaN  3.0  1.0  2.0\n",
      ".             1.0  1.0  1.0  1.0\n",
      "English       NaN  1.0  NaN  NaN\n",
      "Ipsum         NaN  1.0  1.0  NaN\n",
      "It            1.0  NaN  NaN  NaN\n",
      "Lorem         NaN  1.0  1.0  NaN\n",
      "Many          NaN  NaN  1.0  NaN\n",
      "The           NaN  1.0  NaN  NaN\n",
      "Various       NaN  NaN  NaN  1.0\n",
      "a             3.0  1.0  1.0  NaN\n",
      "accident      NaN  NaN  NaN  1.0\n",
      "and           NaN  NaN  2.0  1.0\n",
      "as            NaN  1.0  1.0  NaN\n",
      "at            1.0  NaN  NaN  NaN\n",
      "be            1.0  NaN  NaN  NaN\n",
      "by            1.0  NaN  NaN  1.0\n",
      "content       1.0  1.0  NaN  NaN\n",
      "default       NaN  NaN  1.0  NaN\n",
      "desktop       NaN  NaN  1.0  NaN\n",
      "distracted    1.0  NaN  NaN  NaN\n",
      "distribution  NaN  1.0  NaN  NaN\n",
      "editors       NaN  NaN  1.0  NaN\n",
      "established   1.0  NaN  NaN  NaN\n",
      "evolved       NaN  NaN  NaN  1.0\n",
      "fact          1.0  NaN  NaN  NaN\n",
      "...           ...  ...  ...  ...\n",
      "normal        NaN  1.0  NaN  NaN\n",
      "now           NaN  NaN  1.0  NaN\n",
      "of            1.0  2.0  NaN  NaN\n",
      "on            NaN  NaN  NaN  1.0\n",
      "opposed       NaN  1.0  NaN  NaN\n",
      "over          NaN  NaN  NaN  1.0\n",
      "packages      NaN  NaN  1.0  NaN\n",
      "page          1.0  NaN  1.0  NaN\n",
      "point         NaN  1.0  NaN  NaN\n",
      "publishing    NaN  NaN  1.0  NaN\n",
      "purpose       NaN  NaN  NaN  1.0\n",
      "readable      1.0  1.0  NaN  NaN\n",
      "reader        1.0  NaN  NaN  NaN\n",
      "search        NaN  NaN  1.0  NaN\n",
      "sites         NaN  NaN  1.0  NaN\n",
      "sometimes     NaN  NaN  NaN  2.0\n",
      "still         NaN  NaN  1.0  NaN\n",
      "text          NaN  NaN  1.0  NaN\n",
      "that          1.0  1.0  NaN  NaN\n",
      "the           1.0  NaN  NaN  2.0\n",
      "their         NaN  NaN  2.0  NaN\n",
      "to            NaN  1.0  NaN  NaN\n",
      "uncover       NaN  NaN  1.0  NaN\n",
      "use           NaN  NaN  1.0  NaN\n",
      "using         NaN  2.0  NaN  NaN\n",
      "versions      NaN  NaN  NaN  1.0\n",
      "web           NaN  NaN  2.0  NaN\n",
      "when          1.0  NaN  NaN  NaN\n",
      "will          1.0  NaN  1.0  NaN\n",
      "years         NaN  NaN  NaN  1.0\n",
      "\n",
      "[82 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "data = \"\"\"\n",
    "It is a long established fact that a reader will be distracted by the \n",
    "readable content of a page when looking at its layout. The point of using\n",
    "Lorem Ipsum is that it has a more-or-less normal distribution of letters, \n",
    "as opposed to using 'Content here, content here', making it look like readable \n",
    "English. Many desktop publishing packages and web page editors now use Lorem \n",
    "Ipsum as their default model text, and a search for 'lorem ipsum' will uncover \n",
    "many web sites still in their infancy. Various versions have evolved over the \n",
    "years, sometimes by accident, sometimes on purpose (injected humour and the like).\n",
    "\"\"\"\n",
    "sent_tokens = sent_tokenize(data)\n",
    "paracount = dict()\n",
    "k=1\n",
    "for line in sent_tokens:\n",
    "    wordtokens = word_tokenize(line)\n",
    "#     linecount = len(sent_tokens)\n",
    "    count = dict()\n",
    "    for word in wordtokens:\n",
    "        if word in count:\n",
    "            count[word] +=1\n",
    "        else:\n",
    "            count[word] = 1\n",
    "    paracount[k] = count\n",
    "    k+=1\n",
    "# print(paracount)\n",
    "pdf = pd.DataFrame(paracount)\n",
    "print(pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "  \n",
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\"\n",
    "  \n",
    "stop_words = set(stopwords.words('English')) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'between', 'me', 'can', \"you're\", 'what', 's', 'ma', 'on', 'his', 'will', 'mightn', \"hasn't\", 'or', 'do', 'such', 'him', 're', 'were', 'you', 'theirs', 'against', 'yours', 'needn', 'most', 'shan', 'am', 'weren', 'itself', \"mightn't\", 'doing', 'shouldn', 'which', 'he', 'all', 'she', 'with', 'each', 'again', 'haven', 'about', 'very', 'doesn', \"doesn't\", 'yourselves', \"it's\", \"don't\", 'below', 'ain', \"shan't\", 'a', 'have', 'from', 'being', 'wasn', 'down', 'they', 'we', 'who', 'll', 'those', 'by', 'so', 'up', \"haven't\", 'any', 'o', 'couldn', 'when', 'himself', 'into', 'through', 'it', 'before', \"you've\", \"couldn't\", 'ours', 'where', 'won', 'now', 'is', 'off', 'an', 'just', 've', 'myself', 'our', 'further', 'in', 'here', \"shouldn't\", 'as', 'both', 'only', 'isn', \"mustn't\", 'too', \"aren't\", 'if', 'some', 'mustn', 'themselves', 'aren', 'are', \"wouldn't\", 'its', 'been', 'after', 'under', 'hadn', 'my', \"you'll\", 'these', 'how', 'i', 'that', 'own', 'at', 'the', 'yourself', 'was', 'while', 'for', \"won't\", 'd', 'and', \"hadn't\", 'no', \"you'd\", 'their', 'out', \"that'll\", 'be', \"should've\", 'wouldn', 'but', 'to', 'other', 'hasn', \"didn't\", 'does', 'why', 'nor', 'ourselves', 'her', 'y', 'above', \"isn't\", 'then', 'once', 'had', 'your', 'few', 'should', 'there', 't', \"needn't\", 'until', 'of', 'herself', 'same', 'than', 'because', 'did', 'has', 'over', 'hers', 'them', 'having', 'm', 'whom', \"she's\", 'didn', 'not', \"wasn't\", \"weren't\", 'during', 'more', 'don', 'this'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "word_tokens = word_tokenize(example_sent) \n",
    "  \n",
    "# filtered_sentence = [w for w in word_tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence = list()\n",
    "  \n",
    "for w in word_tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence.append(w) \n",
    "\n",
    "print(word_tokens) \n",
    "print(filtered_sentence) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part Of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abbreviation\tMeaning\n",
    "CC\tcoordinating conjunction\n",
    "CD\tcardinal digit\n",
    "DT\tdeterminer\n",
    "EX\texistential there\n",
    "FW\tforeign word\n",
    "IN\tpreposition/subordinating conjunction\n",
    "JJ\tadjective (large)\n",
    "JJR\tadjective, comparative (larger)\n",
    "JJS\tadjective, superlative (largest)\n",
    "LS\tlist market\n",
    "MD\tmodal (could, will)\n",
    "NN\tnoun, singular (cat, tree)\n",
    "NNS\tnoun plural (desks)\n",
    "NNP\tproper noun, singular (sarah)\n",
    "NNPS\tproper noun, plural (indians or americans)\n",
    "PDT\tpredeterminer (all, both, half)\n",
    "POS\tpossessive ending (parent\\ 's)\n",
    "PRP\tpersonal pronoun (hers, herself, him,himself)\n",
    "PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "RB\tadverb (occasionally, swiftly)\n",
    "RBR\tadverb, comparative (greater)\n",
    "RBS\tadverb, superlative (biggest)\n",
    "RP\tparticle (about)\n",
    "TO\tinfinite marker (to)\n",
    "UH\tinterjection (goodbye)\n",
    "VB\tverb (ask)\n",
    "VBG\tverb gerund (judging)\n",
    "VBD\tverb past tense (pleaded)\n",
    "VBN\tverb past participle (reunified)\n",
    "VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "WDT\twh-determiner (that, what)\n",
    "WP\twh- pronoun (who)\n",
    "WRB\twh- adverb (how)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Split: ['Natural', 'Language', 'Processing', 'is', 'manipulation', 'or', 'understanding', 'text', 'or', 'speech', 'by', 'any', 'software', 'or', 'machine.', 'An', 'analogy', 'is', 'that', 'humans', 'interact,', 'understand', 'each', 'other', 'views,', 'and', 'respond', 'with', 'the', 'appropriate', 'answer.']\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Token: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('manipulation', 'NN'), ('or', 'CC'), ('understanding', 'JJ'), ('text', 'NN'), ('or', 'CC'), ('speech', 'NN'), ('by', 'IN'), ('any', 'DT'), ('software', 'NN'), ('or', 'CC'), ('machine.', 'VB'), ('An', 'DT'), ('analogy', 'NN'), ('is', 'VBZ'), ('that', 'IN'), ('humans', 'NNS'), ('interact,', 'JJ'), ('understand', 'VBP'), ('each', 'DT'), ('other', 'JJ'), ('views,', 'NN'), ('and', 'CC'), ('respond', 'NN'), ('with', 'IN'), ('the', 'DT'), ('appropriate', 'JJ'), ('answer.', 'NN')]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Regex: chunk.RegexpParser with 1 stages:\n",
      "RegexpChunkParser with 1 rules:\n",
      "       <ChunkRule: '<NN.?>*<VBD.?>*<JJ.?>*<CC>?'>\n",
      "----------------------------------------------------------------------------------------------------\n",
      "After Chunking (S\n",
      "  (mychunk Natural/JJ)\n",
      "  (mychunk Language/NNP Processing/NNP)\n",
      "  is/VBZ\n",
      "  (mychunk manipulation/NN or/CC)\n",
      "  (mychunk understanding/JJ)\n",
      "  (mychunk text/NN or/CC)\n",
      "  (mychunk speech/NN)\n",
      "  by/IN\n",
      "  any/DT\n",
      "  (mychunk software/NN or/CC)\n",
      "  machine./VB\n",
      "  An/DT\n",
      "  (mychunk analogy/NN)\n",
      "  is/VBZ\n",
      "  that/IN\n",
      "  (mychunk humans/NNS interact,/JJ)\n",
      "  understand/VBP\n",
      "  each/DT\n",
      "  (mychunk other/JJ)\n",
      "  (mychunk views,/NN and/CC)\n",
      "  (mychunk respond/NN)\n",
      "  with/IN\n",
      "  the/DT\n",
      "  (mychunk appropriate/JJ)\n",
      "  (mychunk answer./NN))\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import RegexpParser\n",
    "text =\"\"\"Natural Language Processing is manipulation or understanding text\n",
    "or speech by any software or machine. An analogy is that humans interact, \n",
    "understand each other views, and respond with the appropriate answer.\"\"\"\n",
    "text = text.split()\n",
    "print(\"After Split:\",text)\n",
    "print('-'*100)\n",
    "tokens_tag = pos_tag(text)\n",
    "print(\"After Token:\",tokens_tag)\n",
    "print('-'*100)\n",
    "patterns= \"\"\"mychunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\"\n",
    "chunker = RegexpParser(patterns)\n",
    "print(\"After Regex:\",chunker)\n",
    "print('-'*100)\n",
    "output = chunker.parse(tokens_tag)\n",
    "print(\"After Chunking\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n",
      "wait\n",
      "wait\n",
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
    "ps =PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "natur\n",
      "languag\n",
      "process\n",
      "is\n",
      "manipul\n",
      "or\n",
      "understand\n",
      "text\n",
      "or\n",
      "speech\n",
      "by\n",
      "ani\n",
      "softwar\n",
      "or\n",
      "machin\n",
      ".\n",
      "An\n",
      "analog\n",
      "is\n",
      "that\n",
      "human\n",
      "interact\n",
      ",\n",
      "understand\n",
      "each\n",
      "other\n",
      "view\n",
      ",\n",
      "and\n",
      "respond\n",
      "with\n",
      "the\n",
      "appropri\n",
      "answer\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence=\"Natural Language Processing is manipulation or understanding text or speech by any software or machine. An analogy is that humans interact, understand each other views, and respond with the appropriate answer.\"\n",
    "words = word_tokenize(sentence)\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "    rootWord=ps.stem(w)\n",
    "    print(rootWord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemming for studies is studi\n",
      "Stemming for studying is studi\n",
      "Stemming for cries is cri\n",
      "Stemming for cry is cri\n"
     ]
    }
   ],
   "source": [
    "porter_stemmer  = PorterStemmer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
